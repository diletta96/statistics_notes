Okay, so in this lecture, we're going to say a little bit more about where data come from and why that's so important in thinking about the statistical approaches that will take to analyzing data. So going into a data analysis, we really want to have a good sense of where the data come from and there are some important questions that we need to ask when thinking about what process generated the data. So, we need to think about the fact that there are different types of data that we may ultimately be collecting in any kind of research study. In general, there are two key types of data: one is organic or process data, and we'll talk a little bit more about what that means, and the other is more designed data collection. When we think about organic or process data, these are data that are generated by a computerized information system or maybe extracted from video or audio recordings. The common point is that they're generated organically as the result of some process and they're often generated over time. Some common examples would be financial or point of sale transactions or even stock market exchanges where there are many, many events occurring over time and all these different events are recorded resulting in very large data sets. What about Netflix viewing history? You may have heard of competitions where people try to predict what people who are subscribed to Netflix may be interested in watching. Well, Netflix has recordings of your entire viewing history and what you've seen and what you've looked at, this again creates a very big data set, but it arises organically, it arises naturally based on your own personal viewing history and results in a very large set of data. Web browser activity, so when you're surfing the web, you're visiting different websites, those data can be recorded as well. How much time do you spend on individual pages? What pages do you visit? How often do you visit them, and then you multiply all those different events across all the different people surfing the web and you can imagine there's a humongous set of events and transactions that occurred just in browsing the web. Then also sporting events, there are a number of different sports, people are recording different metrics and sporting events, outcomes of sporting events, performance of individuals, you think about all the different sports, all the different teams playing those boards, professional, semi-professional, college, high school, even down to some middle school competitions people are recording data about the outcomes of those events both for teams and individuals, but these data are all arising organically as the result of some process and this forms very large data sets that people may be interested in working with to uncover patterns. Finally, we can think about temperature and pollution sensors. If we're trying to understand problems with pollution in a major city or fluctuations in temperature, researchers might set up weather stations or temperature monitors at different locations and collect data over time. Many measurements per second even in terms of what's happening with the temperature or various measures of pollution in a particular area, again, organic process data that could tell a very interesting story about what might be happening over time. So, all of these different processes that I just introduced, they generate massive quantities of data, and this is where we hear the term big data, when you hear that a lot in the popular lexicon these days, big data really refers to these types of data sets that are coming from organic processes, trends over time where we have many, many transactions or events that we're interested in analyzing, but these data sets are massive. So we could talk about baseball games, meals ordered at a particular McDonald's on a given day, changes in temperature on a given day in a particular city, and then you think about all the different areas where these data might be collected, this just introduces massive, massive data sets and a lot of statisticians and data scientists are working on the more efficient ways that can be used to process these data, it's a very active area of research right now, but we have to deal with these big data sets when thinking about research questions. So given these big data, the processing of the data requires significant computational resources. So, what data scientists do is they mine these data to study trends and uncover interesting relationships, but just getting the data in a format that's suitable for analysis, that process alone requires very significant resources from a computational perspective. That requires a lot of people who are trained in computer science and statistical methodology to really form data sets that are useful for analysis, but this too is a very interesting area to work in, how to make these big data sets ready for analysis. So, the other main type of data that we might be working with is arising from more of a design data collection, so not necessarily a natural organic process, like what happens in baseball games over time, but rather specific studies that are designed to specifically address a particular research objective. So, these could be individuals that are sampled from a population and then interviewed about their opinions on a particular topic and you see the little graphic here, we have a target population that we're interested in talking about, but we draw a sample of individualist because it might be difficult to measure everybody within that target population. So, we design a sample of individuals and then ask questions to those individuals to try to learn something more about the population. You could also think about tweets that are extracted from Twitter and then coded by a research team to analyze how often people might be expressing opinions about a particular topic. This is another example of data collection design where specifically designing a sample or a way to extract a subset of tweets from that overall very large population of tweets that are sent out on a daily basis, but a research study might focus on a very specific set of tweets and then code them using a very specific coding design to analyze how people are expressing opinions and how often those opinions are being expressed. You see the key differences though, these are very rigorously designed data collections as opposed to just large data sets that are rising organically that people might be interested in looking at. So some common features of design data, you might have sampling from populations like I had just mentioned or administration of carefully designed survey questions to small samples from larger populations. These data sets as I mentioned, are generally much smaller compared to organic and process data sets, they're easier to work with from a computational perspective, and the data are collected for very specific reasons in these designed data collections rather than being simple reflections of an ongoing natural process like stock market exchanges. In this specialization, we're going to be working with both types of data, we're going to talk a little bit more about sampling later on in this particular course, but in the different examples that we're looking at, we'll think about both types of data and what we need to think about when working with these different types of data collection. Another important question that arises when thinking about where data come from, so we've talked about do the data come from an organic process where they may be very large in nature and might require some computational resources? Do the data come from a very rigorously design data collection with very specific research objectives? A question that needs to be asked in both cases is are the data i.i.d? You see this kind of notation here i.i.d. For analyzing data regardless of the source of the data, an important question that we need to ask is can the data be considered i.i.d? Now, the first i refers to independent and then the later i.d refers to identically distributed. So, i.i.d stands for independent and identically distributed data. So, what does that really mean? Observations on a variable of interest in the i.i.d case are completely independent of all observations. So, there's no correlation between the different measurements that are collected from different units in the population, different stock market exchanges, different baseball games, different individuals who are responding to a survey, all those observations are independent of all the other observations that might ultimately make up a data set. In addition, all of the values that we actually observe arise from some common statistical distribution, so that's the identically distributed part of i.i.d. So all the observations are independent of all the other observations and the values that we're looking at are all arising from some common statistical distribution, that's where the i.i.d terminology comes from. So, here's a little example: you can think of a bell-shaped curve, we're going to talk a lot about bell-shaped curves in this specialization or normal distributions. Think about final exam scores from a large introduction to statistics class at a university, we can think of those as being independent observations each student is providing their own exam answers, each student is getting graded separately in terms of their performance, those exam scores are independent observations and the values representing those final exam scores, they may follow a natural normal distribution, this kind of bell shaped curve. So, all of those exams scores are independent observations, each student treated independently that are coming from a common normal distribution. If we look at that entire distribution of exam scores, it might look like this kinda bell-shaped curve. Given these i.i.d data, we can estimate the features of that distribution, things like the mean exam score, the variance in the exam scores, extreme percentiles of the exam scores and then make inference about those features. What do we think the overall mean is in the population? Or what do we think the extreme percentiles are in some larger population of students that would take introduction to statistics at this university? In the i.i.d case, we can estimate the features of that distribution with a certain amount of precision so we can determine how much precision those different estimates have assuming that the data are i.i.d. So it's an important assumption that enables particular types of statistical procedures in describing data sets and describing populations. So, what do we do if the data are not i.i.d? So as we just discussed, it's an important assumption that enables certain kinds of statistical procedures and allows us to make statements about populations. But here are some examples of non-i.i.d data. For example, students in the class might cheat off each other, they might sit next to each other and then because they're looking at each other's answers, they might have similar scores, that breaks the assumption of independence that underlies i.i.d data. So, students sitting in the same region tend to look at each other's exams and write down similar answers, they might tend to have similar scores, those scores are dependent on the scores of other students. Males and females might have different means, so what that means is that there are different distributions of exam scores for males and females, so those data are not arising from an identical distribution. Different subgroups of cases may follow different distributions in terms of their observations. Or students coming from the same discussion section in that Introduction to Statistics Course may have similar scores, maybe one graduate student instructor does a better job in their discussion sections, making concepts clear than does a graduate student instructor in another discussion section. Then there's variability between those discussion sections, so like the difference between males and females, those data may not be identically distributed. In these cases, when the data are not i.i.d, we need to account for those dependencies and those differences in the analysis that we're performing, so we may need different analytic procedures when the data are not in fact i.i.d. So when you think about it, this statistical procedures that we'll be talking about, they either assume that the data are i.i.d or they assume that the data are not i.i.d. So, we're going to be talking about these different analytic procedures in these different settings as we move forward in the specialization. But the key point up front is that understanding where the data come from is an important piece of information for determining what analytic procedures to use. So, some important notes: we need to ask the question, can we apply statistical procedures to the data that assume i.i.d data? So, is an important question to ask right up front before you get into an analysis, and we always need to consider where the data came from, did they come from some organic process? Could that process have introduced dependencies in the data? Did they come from a design data collection where we may need to account for the features of that design and we ultimately do the analysis up front before we started analyzing data? We always want to consider where the data came from. So later on in this course, we're going to say more, we'll provide more details about design data collection in the i.i.d idea when we start talking about sampling and the analytic procedures that we would use in the case of design samples.